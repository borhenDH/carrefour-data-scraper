{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a924583e",
   "metadata": {},
   "source": [
    "# Carrefour Product Scraper\n",
    "\n",
    "This notebook scrapes product information from Carrefour's website, including:\n",
    "- Product image\n",
    "- Product link\n",
    "- Product name\n",
    "- Price (current and original)\n",
    "- Labels/tags\n",
    "- Product description\n",
    "- Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96eed476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# Add this to your existing imports cell\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7fff6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options for Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ded1284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this improved scrape_page function that's more resilient to failures\n",
    "def scrape_page(url, max_retries=3, initial_delay=3):\n",
    "    \"\"\"\n",
    "    Scrape product information from a specific page with built-in retry logic\n",
    "    and improved wait strategies\n",
    "    \"\"\"\n",
    "    retry_count = 0\n",
    "    backoff_factor = 1.5\n",
    "    current_delay = initial_delay\n",
    "    \n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            # Clear cookies and cache before loading the page\n",
    "            if retry_count > 0:\n",
    "                driver.delete_all_cookies()\n",
    "                \n",
    "            # Load the page\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Use a longer wait on first attempt and after failures\n",
    "            wait_time = 10 if retry_count == 0 else 15\n",
    "            \n",
    "            # Wait for the first product card with a longer timeout\n",
    "            try:\n",
    "                WebDriverWait(driver, wait_time).until(\n",
    "                    EC.presence_of_all_elements_located((By.CLASS_NAME, \"category-categoryItem-7pb\"))\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # If we can't find product cards, check if there's any content\n",
    "                # Maybe we're on an empty page or error page\n",
    "                body_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "                if \"No products found\" in body_text or \"Please try again\" in body_text:\n",
    "                    print(f\"Page indicates no products available\")\n",
    "                    return []\n",
    "                else:\n",
    "                    raise e  # Re-raise if it's not an empty results page\n",
    "            \n",
    "            # Let the page fully render and JS execute\n",
    "            time.sleep(current_delay)\n",
    "            \n",
    "            # Scroll down to ensure lazy-loaded content is loaded\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Get the page source and parse with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Find all product cards\n",
    "            product_cards = soup.find_all(\"div\", class_=\"category-categoryItem-7pb\")\n",
    "            \n",
    "            # If we found no products, double-check with Selenium\n",
    "            if not product_cards:\n",
    "                # Try direct Selenium approach as fallback\n",
    "                try:\n",
    "                    selenium_cards = driver.find_elements(By.CLASS_NAME, \"category-categoryItem-7pb\")\n",
    "                    if selenium_cards:\n",
    "                        print(f\"BeautifulSoup couldn't find cards but Selenium found {len(selenium_cards)}\")\n",
    "                        # If Selenium found cards but BS4 didn't, we need to use Selenium extraction\n",
    "                        return extract_products_with_selenium(driver)\n",
    "                except Exception as selenium_err:\n",
    "                    print(f\"Selenium fallback also failed: {selenium_err}\")\n",
    "            \n",
    "            products = []\n",
    "            \n",
    "            for card in product_cards:\n",
    "                product = {}\n",
    "                \n",
    "                # Extract product link\n",
    "                link_elem = card.find(\"a\", class_=\"galleryItemExtend-images-Rt2\")\n",
    "                if link_elem and link_elem.has_attr('href'):\n",
    "                    product['link'] = \"https://www.carrefour.tn\" + link_elem['href']\n",
    "                \n",
    "                # Extract product image\n",
    "                image_elem = card.find(\"img\", class_=\"image-loaded-QS8\")\n",
    "                if image_elem and image_elem.has_attr('src'):\n",
    "                    product['image_url'] = \"https://www.carrefour.tn\" + image_elem['src'].split('?')[0]\n",
    "                \n",
    "                # Extract product name\n",
    "                name_elem = card.find(\"span\", class_=\"item-name-LPg\")\n",
    "                if name_elem:\n",
    "                    product['name'] = name_elem.text.strip()\n",
    "                \n",
    "                # Extract current price\n",
    "                price_container = card.find(\"div\", class_=\"item-priceReduction-hBy\")\n",
    "                if price_container:\n",
    "                    integer = price_container.find(\"span\", class_=\"item-miniInteger-NhR\")\n",
    "                    decimal = price_container.find(\"span\", class_=\"item-miniDecimal-Cwx\")\n",
    "                    currency = price_container.find(\"span\", class_=\"item-miniCurrency-sAq\")\n",
    "                    \n",
    "                    if integer and decimal and currency:\n",
    "                        product['price'] = f\"{integer.text}.{decimal.text} {currency.text}\"\n",
    "                \n",
    "                # Extract old price if available\n",
    "                old_price_container = card.find(\"div\", class_=\"item-oldPrice-x2a\")\n",
    "                if old_price_container:\n",
    "                    integer = old_price_container.find(\"span\", class_=\"item-miniInteger-NhR\")\n",
    "                    decimal = old_price_container.find(\"span\", class_=\"item-miniDecimal-Cwx\")\n",
    "                    currency = old_price_container.find(\"span\", class_=\"item-miniCurrency-sAq\")\n",
    "                    \n",
    "                    if integer and decimal and currency:\n",
    "                        product['old_price'] = f\"{integer.text}.{decimal.text} {currency.text}\"\n",
    "                else:\n",
    "                    product['old_price'] = None\n",
    "                \n",
    "                # Extract labels/tags if available\n",
    "                labels_container = card.find(\"div\", class_=\"productLabels-root--7k\")\n",
    "                if labels_container and labels_container.find_all(\"div\", class_=\"label-root-K4m\"):\n",
    "                    product['label'] = \"Promo\"  # Based on your example, this seems to be a promotion label\n",
    "                else:\n",
    "                    product['label'] = None\n",
    "                \n",
    "                # Extract brand\n",
    "                brand_elem = card.find(\"div\", class_=\"item-carrefourLabel-AeJ\")\n",
    "                if brand_elem:\n",
    "                    product['brand'] = brand_elem.text.strip()\n",
    "                \n",
    "                # Extract description if available\n",
    "                desc_elem = card.find(\"div\", class_=\"item-description-oxA\")\n",
    "                if desc_elem and desc_elem.find(\"div\", class_=\"richContent-root-Ddk\"):\n",
    "                    product['description'] = desc_elem.find(\"div\", class_=\"richContent-root-Ddk\").text.strip()\n",
    "                else:\n",
    "                    product['description'] = None\n",
    "                \n",
    "                products.append(product)\n",
    "            \n",
    "            # If we successfully scraped products, return them\n",
    "            if products:\n",
    "                print(f\"Successfully scraped {len(products)} products\")\n",
    "                return products\n",
    "            else:\n",
    "                # If no products found but no error thrown, we might be at the end\n",
    "                print(\"No products found on page, might be end of pagination\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\"Error during page scraping (attempt {retry_count}/{max_retries}): {str(e)}\")\n",
    "            \n",
    "            if retry_count <= max_retries:\n",
    "                print(f\"Waiting {current_delay}s before retrying...\")\n",
    "                time.sleep(current_delay)\n",
    "                current_delay *= backoff_factor  # Increase delay for next retry\n",
    "            else:\n",
    "                print(f\"Failed to scrape after {max_retries} attempts\")\n",
    "                # Return empty list on complete failure\n",
    "                return []\n",
    "    \n",
    "    return []\n",
    "\n",
    "def extract_products_with_selenium(driver):\n",
    "    \"\"\"Fallback extraction method using Selenium directly instead of BeautifulSoup\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    try:\n",
    "        # Find all product cards using Selenium\n",
    "        cards = driver.find_elements(By.CLASS_NAME, \"category-categoryItem-7pb\")\n",
    "        \n",
    "        for card in cards:\n",
    "            product = {}\n",
    "            \n",
    "            # Extract with more defensive error handling for each element\n",
    "            try:\n",
    "                link_elem = card.find_element(By.CLASS_NAME, \"galleryItemExtend-images-Rt2\")\n",
    "                product['link'] = \"https://www.carrefour.tn\" + link_elem.get_attribute('href')\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                image_elem = card.find_element(By.CLASS_NAME, \"image-loaded-QS8\")\n",
    "                src = image_elem.get_attribute('src')\n",
    "                if src:\n",
    "                    product['image_url'] = \"https://www.carrefour.tn\" + src.split('?')[0]\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                name_elem = card.find_element(By.CLASS_NAME, \"item-name-LPg\")\n",
    "                product['name'] = name_elem.text.strip()\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            # For price, brand, description, etc. - similar extraction with try/except blocks\n",
    "            \n",
    "            if 'name' in product and 'link' in product:  # Only add if we at least have name and link\n",
    "                products.append(product)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Selenium extraction failed: {e}\")\n",
    "        \n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "214da418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dismiss_cookie_banner(driver, max_attempts=3):\n",
    "    \"\"\"\n",
    "    Find and dismiss the cookie banner/popup if present\n",
    "    \"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Look for the \"Continue without accepting\" button\n",
    "            cookie_button = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, \"cookies-notacceptBtn-wQ1\"))\n",
    "            )\n",
    "            \n",
    "            print(\"Found cookie consent banner, attempting to dismiss...\")\n",
    "            \n",
    "            # Scroll the button into view\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", cookie_button)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Try different click methods\n",
    "            try:\n",
    "                # Direct click\n",
    "                cookie_button.click()\n",
    "            except:\n",
    "                # JavaScript click as fallback\n",
    "                driver.execute_script(\"arguments[0].click();\", cookie_button)\n",
    "            \n",
    "            print(\"Successfully dismissed cookie banner\")\n",
    "            time.sleep(2)  # Wait for banner to disappear\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If we can't find the button or clicking fails, it might not be present\n",
    "            if attempt == max_attempts - 1:\n",
    "                print(\"No cookie banner found or failed to dismiss it\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5862e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_load_more_button(driver, max_attempts=2):\n",
    "    \"\"\"Optimized button click function - only check for cookies banner once\"\"\"\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Find the button more efficiently with a shorter timeout\n",
    "            load_more_button = WebDriverWait(driver, 3).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"category-buttonsPag-jTw\"))\n",
    "            )\n",
    "            \n",
    "            if \"PRODUITS SUIVANTS\" not in load_more_button.text:\n",
    "                return False\n",
    "                \n",
    "            # Directly use JavaScript click (fastest method)\n",
    "            driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "            \n",
    "            # Wait just enough time for page to start loading new content\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                lambda d: len(d.find_elements(By.CLASS_NAME, \"category-categoryItem-7pb\")) > \n",
    "                          int(d.execute_script(\"return window._last_product_count || 0\"))\n",
    "            )\n",
    "            \n",
    "            # Store current count for next comparison\n",
    "            driver.execute_script(\"window._last_product_count = document.querySelectorAll('.category-categoryItem-7pb').length;\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_attempts - 1:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(category_name, products, current_attempt, save_dir):\n",
    "    \"\"\"Save current scraping progress to files\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the products to CSV\n",
    "    if products:\n",
    "        df = pd.DataFrame(products)\n",
    "        csv_path = os.path.join(save_dir, f\"{category_name}_products.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save progress information\n",
    "    progress_info = {\n",
    "        \"category\": category_name,\n",
    "        \"last_updated\": datetime.now().isoformat(),\n",
    "        \"current_attempt\": current_attempt,\n",
    "        \"products_count\": len(products),\n",
    "        \"status\": \"in_progress\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"progress.json\"), 'w') as f:\n",
    "        json.dump(progress_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Progress saved: {len(products)} products from {category_name} (attempt {current_attempt})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be784254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_category_with_load_more(base_url, category_name=None, max_attempts=100):\n",
    "    \"\"\"Optimized category scraper with better performance\"\"\"\n",
    "    if not category_name:\n",
    "        parsed_url = urlparse(base_url)\n",
    "        category_name = os.path.splitext(os.path.basename(parsed_url.path))[0]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting to scrape category: {category_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    save_dir = os.path.join(\"carrefour_data\", category_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize variables\n",
    "    driver.get(base_url)\n",
    "    all_products = []\n",
    "    current_attempt = 0\n",
    "    previous_count = 0\n",
    "    no_new_products_streak = 0\n",
    "    product_links_set = set()  # For faster duplicate checking\n",
    "    \n",
    "    # Only check for cookie banner once at the beginning\n",
    "    dismiss_cookie_banner(driver)\n",
    "    \n",
    "    # Wait for initial products with shorter timeout\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"category-categoryItem-7pb\"))\n",
    "        )\n",
    "        \n",
    "        # Initial scroll to trigger lazy loading\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)  # Reduced wait time\n",
    "        \n",
    "        print(\"Initial page loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading initial page: {e}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        while current_attempt < max_attempts:\n",
    "            # Extract only the product links first (much faster than full extraction)\n",
    "            product_elements = driver.find_elements(By.CLASS_NAME, \"category-categoryItem-7pb\")\n",
    "            current_product_count = len(product_elements)\n",
    "            print(f\"Found {current_product_count} products on the page\")\n",
    "            \n",
    "            # Check if we made progress\n",
    "            if current_product_count <= previous_count:\n",
    "                no_new_products_streak += 1\n",
    "                if no_new_products_streak >= 2:  # Reduced from 3 to 2\n",
    "                    print(\"No new products after multiple attempts. All products likely scraped.\")\n",
    "                    break\n",
    "            else:\n",
    "                no_new_products_streak = 0\n",
    "            \n",
    "            # Only process products if we have more than before or it's the first run\n",
    "            if current_product_count > previous_count or not all_products:\n",
    "                # Extract product data more efficiently - only process new products\n",
    "                start_idx = min(previous_count, current_product_count)  # Start from where we left off\n",
    "                \n",
    "                # Use the page source only once instead of repeatedly\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                product_cards = soup.find_all(\"div\", class_=\"category-categoryItem-7pb\")\n",
    "                \n",
    "                # Process only the new cards\n",
    "                new_cards = product_cards[start_idx:]\n",
    "                new_products = []\n",
    "                \n",
    "                for card in new_cards:\n",
    "                    product = {}\n",
    "                    \n",
    "                    # Extract product link first to check if we've seen it\n",
    "                    link_elem = card.find(\"a\", class_=\"galleryItemExtend-images-Rt2\")\n",
    "                    if link_elem and link_elem.has_attr('href'):\n",
    "                        product_link = \"https://www.carrefour.tn\" + link_elem['href']\n",
    "                        \n",
    "                        # Skip if we've already processed this product\n",
    "                        if product_link in product_links_set:\n",
    "                            continue\n",
    "                            \n",
    "                        product_links_set.add(product_link)\n",
    "                        product['link'] = product_link\n",
    "                        \n",
    "                        # Now extract other product attributes\n",
    "                        # [Rest of your product extraction code]\n",
    "                        # Extract product image\n",
    "                        image_elem = card.find(\"img\", class_=\"image-loaded-QS8\")\n",
    "                        if image_elem and image_elem.has_attr('src'):\n",
    "                            product['image_url'] = \"https://www.carrefour.tn\" + image_elem['src'].split('?')[0]\n",
    "                        \n",
    "                        # Extract product name\n",
    "                        name_elem = card.find(\"span\", class_=\"item-name-LPg\")\n",
    "                        if name_elem:\n",
    "                            product['name'] = name_elem.text.strip()\n",
    "                        \n",
    "                        # Extract current price\n",
    "                        price_container = card.find(\"div\", class_=\"item-priceReduction-hBy\")\n",
    "                        if price_container:\n",
    "                            integer = price_container.find(\"span\", class_=\"item-miniInteger-NhR\")\n",
    "                            decimal = price_container.find(\"span\", class_=\"item-miniDecimal-Cwx\")\n",
    "                            currency = price_container.find(\"span\", class_=\"item-miniCurrency-sAq\")\n",
    "                            \n",
    "                            if integer and decimal and currency:\n",
    "                                product['price'] = f\"{integer.text}.{decimal.text} {currency.text}\"\n",
    "                        \n",
    "                        # Extract old price if available\n",
    "                        old_price_container = card.find(\"div\", class_=\"item-oldPrice-x2a\")\n",
    "                        if old_price_container:\n",
    "                            integer = old_price_container.find(\"span\", class_=\"item-miniInteger-NhR\")\n",
    "                            decimal = old_price_container.find(\"span\", class_=\"item-miniDecimal-Cwx\")\n",
    "                            currency = old_price_container.find(\"span\", class_=\"item-miniCurrency-sAq\")\n",
    "                            \n",
    "                            if integer and decimal and currency:\n",
    "                                product['old_price'] = f\"{integer.text}.{decimal.text} {currency.text}\"\n",
    "                        else:\n",
    "                            product['old_price'] = None\n",
    "                        \n",
    "                        # Extract labels/tags\n",
    "                        labels_container = card.find(\"div\", class_=\"productLabels-root--7k\")\n",
    "                        if labels_container and labels_container.find_all(\"div\", class_=\"label-root-K4m\"):\n",
    "                            product['label'] = \"Promo\"\n",
    "                        else:\n",
    "                            product['label'] = None\n",
    "                        \n",
    "                        # Extract brand\n",
    "                        brand_elem = card.find(\"div\", class_=\"item-carrefourLabel-AeJ\")\n",
    "                        if brand_elem:\n",
    "                            product['brand'] = brand_elem.text.strip()\n",
    "                        \n",
    "                        # Extract description\n",
    "                        desc_elem = card.find(\"div\", class_=\"item-description-oxA\")\n",
    "                        if desc_elem and desc_elem.find(\"div\", class_=\"richContent-root-Ddk\"):\n",
    "                            product['description'] = desc_elem.find(\"div\", class_=\"richContent-root-Ddk\").text.strip()\n",
    "                        else:\n",
    "                            product['description'] = None\n",
    "                            \n",
    "                        new_products.append(product)\n",
    "                \n",
    "                # Add new products to our collection\n",
    "                if new_products:\n",
    "                    all_products.extend(new_products)\n",
    "                    print(f\"Added {len(new_products)} new unique products\")\n",
    "                else:\n",
    "                    no_new_products_streak += 1  # Increment if no new products found\n",
    "            \n",
    "            # Save progress less frequently - only every 5 attempts instead of 3\n",
    "            if current_attempt % 5 == 0:\n",
    "                save_progress(category_name, all_products, current_attempt + 1, save_dir)\n",
    "            \n",
    "            # Click the \"Load More\" button\n",
    "            button_clicked = click_load_more_button(driver)\n",
    "            if button_clicked:\n",
    "                print(\"Successfully clicked the 'Load More' button\")\n",
    "                # Update counters\n",
    "                previous_count = current_product_count\n",
    "                current_attempt += 1\n",
    "                \n",
    "                # Variable delay based on network conditions (reduced overall)\n",
    "                delay = 1 + (current_attempt % 2)  # Either 1 or 2 seconds\n",
    "                print(f\"Waiting {delay} seconds before continuing...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(\"Could not click 'Load More' button or it doesn't exist. Ending scraping.\")\n",
    "                break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during category scraping: {e}\")\n",
    "    finally:\n",
    "        # Always save final progress\n",
    "        save_progress(category_name, all_products, current_attempt + 1, save_dir)\n",
    "    \n",
    "    print(f\"Completed scraping category {category_name}: {len(all_products)} total products\")\n",
    "    return all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "214a354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous summary data with 27 categories\n",
      "Starting scraping job at 2025-05-06 18:20:07.149965\n",
      "Skipping soins-solaires - already completed\n",
      "Skipping nos-recettes - already completed\n",
      "Skipping le-marche - already completed\n",
      "Skipping surgeles - already completed\n",
      "Skipping cremerie-et-produits-laitiers - already completed\n",
      "Skipping boissons - already completed\n",
      "Skipping epicerie-sucree - already completed\n",
      "Skipping bio-sans-gluten-et-dietetique - already completed\n",
      "Skipping entretien-et-nettoyage - already completed\n",
      "Skipping hygiene-et-beaute - already completed\n",
      "Skipping bebe - already completed\n",
      "Skipping animalerie - already completed\n",
      "Skipping jeux-et-jouets - already completed\n",
      "Skipping cuisine - already completed\n",
      "Skipping gros-electromenager - already completed\n",
      "Skipping image-et-son - already completed\n",
      "Skipping informatique - already completed\n",
      "Skipping smartphones-et-objets-connectes - already completed\n",
      "Skipping entretien-de-la-maison - already completed\n",
      "Skipping beaute-et-sante - already completed\n",
      "Skipping maison-et-decoration - already completed\n",
      "Skipping brico-et-auto - already completed\n",
      "Skipping jardin-et-amenagement-dexterieur - already completed\n",
      "Skipping mode-et-bagagerie - already completed\n",
      "Skipping bebe-11 - already completed\n",
      "Skipping sport-et-loisirs - already completed\n",
      "Skipping culture-et-fournitures-scolaires - already completed\n",
      "Scraping job completed at 2025-05-06 18:20:07.154610. Total duration: 0:00:00.004645. Total products: 8666\n",
      "\n",
      "Scraping Summary:\n",
      "--------------------------------------------------\n",
      "✓ soins-solaires: 53 products\n",
      "✓ nos-recettes: 24 products\n",
      "✓ le-marche: 913 products\n",
      "✓ surgeles: 242 products\n",
      "✓ cremerie-et-produits-laitiers: 598 products\n",
      "✓ boissons: 279 products\n",
      "✓ epicerie-sucree: 1422 products\n",
      "✓ bio-sans-gluten-et-dietetique: 120 products\n",
      "✓ entretien-et-nettoyage: 1144 products\n",
      "✓ hygiene-et-beaute: 1410 products\n",
      "✓ bebe: 172 products\n",
      "✓ animalerie: 414 products\n",
      "✓ jeux-et-jouets: 235 products\n",
      "✓ cuisine: 0 products\n",
      "✓ gros-electromenager: 342 products\n",
      "✓ image-et-son: 120 products\n",
      "✓ informatique: 98 products\n",
      "✓ smartphones-et-objets-connectes: 71 products\n",
      "✓ entretien-de-la-maison: 38 products\n",
      "✓ beaute-et-sante: 77 products\n",
      "✓ maison-et-decoration: 0 products\n",
      "✓ brico-et-auto: 180 products\n",
      "✓ jardin-et-amenagement-dexterieur: 405 products\n",
      "✓ mode-et-bagagerie: 188 products\n",
      "✓ bebe-11: 31 products\n",
      "✓ sport-et-loisirs: 60 products\n",
      "✓ culture-et-fournitures-scolaires: 30 products\n",
      "--------------------------------------------------\n",
      "Total categories: 27\n",
      "Completed: 27\n",
      "Failed: 0\n",
      "Total products scraped: 8666\n"
     ]
    }
   ],
   "source": [
    "# Add this main execution cell to replace your previous execution code\n",
    "# List of category URLs to scrape\n",
    "category_urls = [\n",
    "    \"https://www.carrefour.tn/soins-solaires.html\",\n",
    "    \"https://www.carrefour.tn/nos-recettes.html\",\n",
    "    \"https://www.carrefour.tn/le-marche.html\",\n",
    "    \"https://www.carrefour.tn/surgeles.html\",\n",
    "    \"https://www.carrefour.tn/cremerie-et-produits-laitiers.html\",\n",
    "    \"https://www.carrefour.tn/boissons.html\",\n",
    "    \"https://www.carrefour.tn/epicerie-sucree.html\",\n",
    "    \"https://www.carrefour.tn/bio-sans-gluten-et-dietetique.html\",\n",
    "    \"https://www.carrefour.tn/entretien-et-nettoyage.html\",\n",
    "    \"https://www.carrefour.tn/hygiene-et-beaute.html\",\n",
    "    \"https://www.carrefour.tn/bebe.html\",\n",
    "    \"https://www.carrefour.tn/animalerie.html\",\n",
    "    \"https://www.carrefour.tn/jeux-et-jouets.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/cuisine.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/gros-electromenager.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/image-et-son.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/informatique.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/smartphones-et-objets-connectes.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/entretien-de-la-maison.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/beaute-et-sante.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/maison-et-decoration.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/brico-et-auto.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/jardin-et-amenagement-dexterieur.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/mode-et-bagagerie.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/bebe-11.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/sport-et-loisirs.html\",\n",
    "    \"https://www.carrefour.tn/maison-et-loisirs/culture-et-fournitures-scolaires.html\"\n",
    "]\n",
    "\n",
    "# Main execution with browser restart after each category\n",
    "def main():\n",
    "    os.makedirs(\"carrefour_data\", exist_ok=True)\n",
    "    \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        filename=\"carrefour_scraper.log\",\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Track overall progress in a summary file\n",
    "    summary_file = os.path.join(\"carrefour_data\", \"summary.json\")\n",
    "    all_categories_data = {}\n",
    "    \n",
    "    if os.path.exists(summary_file):\n",
    "        try:\n",
    "            with open(summary_file, 'r') as f:\n",
    "                all_categories_data = json.load(f)\n",
    "            print(f\"Loaded previous summary data with {len(all_categories_data)} categories\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading summary file: {e}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    logging.info(f\"Starting scraping job at {start_time}\")\n",
    "    print(f\"Starting scraping job at {start_time}\")\n",
    "    \n",
    "    total_products = 0\n",
    "    \n",
    "    # Process each category with a fresh browser instance\n",
    "    for i, url in enumerate(category_urls):\n",
    "        category_name = os.path.splitext(os.path.basename(urlparse(url).path))[0]\n",
    "        \n",
    "        # Skip if already completed successfully\n",
    "        if category_name in all_categories_data and all_categories_data[category_name].get(\"status\") == \"completed\":\n",
    "            print(f\"Skipping {category_name} - already completed\")\n",
    "            total_products += all_categories_data[category_name].get(\"products_count\", 0)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing category {i+1}/{len(category_urls)}: {category_name}\")\n",
    "        logging.info(f\"Starting category: {category_name} ({url})\")\n",
    "        \n",
    "        # Create a fresh browser instance for each category\n",
    "        driver = None\n",
    "        try:\n",
    "            # Set up Chrome options\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "            chrome_options.add_argument(\"--disable-extensions\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--disable-notifications\")\n",
    "            # These options can help with stability\n",
    "            chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "            chrome_options.add_argument(\"--disable-features=VizDisplayCompositor\")\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            \n",
    "            # Set page load timeout\n",
    "            driver.set_page_load_timeout(30)\n",
    "            \n",
    "            # Scrape this category\n",
    "            products = scrape_category_with_load_more(url, category_name)\n",
    "            \n",
    "            # Update summary data\n",
    "            all_categories_data[category_name] = {\n",
    "                \"url\": url,\n",
    "                \"products_count\": len(products),\n",
    "                \"last_updated\": datetime.now().isoformat(),\n",
    "                \"status\": \"completed\"\n",
    "            }\n",
    "            \n",
    "            total_products += len(products)\n",
    "            \n",
    "            # Save updated summary\n",
    "            with open(summary_file, 'w') as f:\n",
    "                json.dump(all_categories_data, f, indent=2)\n",
    "            \n",
    "            print(f\"Completed category: {category_name} with {len(products)} products\")\n",
    "            logging.info(f\"Completed category: {category_name} with {len(products)} products\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing category {category_name}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            logging.error(error_msg)\n",
    "            \n",
    "            # Mark as failed in summary\n",
    "            all_categories_data[category_name] = {\n",
    "                \"url\": url,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": str(e),\n",
    "                \"last_updated\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Save updated summary even after error\n",
    "            with open(summary_file, 'w') as f:\n",
    "                json.dump(all_categories_data, f, indent=2)\n",
    "                \n",
    "        finally:\n",
    "            # Always close the driver\n",
    "            if driver:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Brief pause between categories\n",
    "            time.sleep(5)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    summary_msg = f\"Scraping job completed at {end_time}. Total duration: {duration}. Total products: {total_products}\"\n",
    "    print(summary_msg)\n",
    "    logging.info(summary_msg)\n",
    "    \n",
    "    return all_categories_data\n",
    "\n",
    "# Run the main execution\n",
    "try:\n",
    "    results = main()\n",
    "    \n",
    "    # Generate a summary report\n",
    "    print(\"\\nScraping Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    total_products = 0\n",
    "    completed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for category, data in results.items():\n",
    "        status = data.get(\"status\", \"unknown\")\n",
    "        count = data.get(\"products_count\", 0)\n",
    "        \n",
    "        if status == \"completed\":\n",
    "            completed += 1\n",
    "            total_products += count\n",
    "            print(f\"✓ {category}: {count} products\")\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"✗ {category}: Failed - {data.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total categories: {len(results)}\")\n",
    "    print(f\"Completed: {completed}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Total products scraped: {total_products}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error in main execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8931a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
